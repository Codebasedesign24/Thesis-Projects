{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMrICKzl1A4V"
      },
      "outputs": [],
      "source": [
        "####NOTEBOOK FOR EXPERIMENT-3-ESG INVESTMENT ANALYSIS USING DATA ANALYTICS\n",
        "As its internal company data , this is relatively small. Although our data set is relatively small, we show how one could distribute the scraping process using a user defined function (UDF), assuming the third-party library `PyPDF2` is available across your Spark environment.\n",
        "import requests\n",
        "import PyPDF2\n",
        "import io\n",
        "\n",
        "@udf('string')\n",
        "def extract_content(url):\n",
        "\n",
        "    # retrieve PDF binary stream\n",
        "    response = requests.get(url)\n",
        "    open_pdf_file = io.BytesIO(response.content)\n",
        "    pdf = PyPDF2.PdfFileReader(open_pdf_file)\n",
        "\n",
        "    # return concatenated content\n",
        "    text = [pdf.getPage(i).extractText() for i in range(0, pdf.getNumPages())]\n",
        "    return \"\\n\".join(text)\n",
        "Beyond regular expressions and fairly complex data cleansing  (reported in the attached notebooks), we also want to leverage more advanced NLP capabilities to tokenise content into grammatically valid sentences. Given the time it takes to load trained NLP pipelines in memory (such as the `spacy` library below), we ensure our model is loaded only once per Spark executor using a PandasUDF strategy as follows.\n",
        "import gensim\n",
        "import spacy\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "\n",
        "@pandas_udf('array', PandasUDFType.SCALAR_ITER)\n",
        "def extract_statements(content_series_iter):\n",
        "\n",
        "    # load spacy model for english only once\n",
        "    spacy.cli.download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # provide process_text function with our loaded NLP model\n",
        "    # clean and tokenize a batch of PDF content\n",
        "    for content_series in content_series_iter:\n",
        "\n",
        "    yield content_series.map(lambda x: process_text(nlp, x))\n",
        "\n",
        "      The 4 banks data which will be fed to NLP and LDA Model and hyperparameters\n",
        "We compute our term frequencies and capture our LDA model and hyperparameters using MLflow experiments tracking.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "import mlflow\n",
        "\n",
        "# compute word frequencies\n",
        "# stop words are common english words + banking related buzzwords\n",
        "word_tf_vectorizer = CountVectorizer(stop_words=stop_words, ngram_range=(1,1))\n",
        "word_tf = word_tf_vectorizer.fit_transform(esg['lemma'])\n",
        "\n",
        "# track experiment on ml-flow\n",
        "with mlflow.start_run(run_name='topic_modeling'):\n",
        "\n",
        "    # Train a LDA model with 9 topics\n",
        "    lda = LDA(random_state = 42, n_components = 9, learning_decay = .3)\n",
        "    lda.fit(word_tf)\n",
        "\n",
        "    # Log model\n",
        "    mlflow.sklearn.log_model(lda, \"model\")\n",
        "    mlflow.log_param('n_components', '9')\n",
        "    mlflow.log_param('learning_decay', '.3')\n",
        "    mlflow.log_metric('perplexity', lda.perplexity(word_tf))\n",
        "\n",
        "\n",
        "\n",
        "Using seaborn visualisation, we can easily flag key differences across our companies (organisations' names redacted). When some organisations would put more focus on valuing employees and promoting diversity and inclusion (such as ORG-21), some seem to be more focused towards ethical investments (ORG-14). As the output of LDA is a probability distribution across our 9 topics instead of one specific theme, we easily unveil the most descriptive ESG initiative for any given organisation using a simple SQL statement and a partitioning function that captures the highest probability for each theme.\n",
        "WITH ranked (\n",
        "    SELECT\n",
        "        e.topic,\n",
        "        e.statement,\n",
        "        e.company,\n",
        "        dense_rank() OVER (\n",
        "            PARTITION BY e.company, e.topic ORDER BY e.probability DESC\n",
        "        ) as rank\n",
        "    FROM esg_reports e\n",
        ")\n",
        "\n",
        "SELECT\n",
        "    t.topic,\n",
        "    t.statement\n",
        "FROM ranked t\n",
        "WHERE t.company = 'goldman sachs'\n",
        "AND t.rank = 1\n",
        "This SQL statement provides us with a NLP generated executive summary for Goldman Sachs (see original report), summarising a complex 70+ pages long document into 9 ESG initiatives / actions\n",
        "\n",
        "we easily unveil the most descriptive ESG initiative for any given organisation using a simple SQL statement and a partitioning function that captures the highest probability for each theme.\n",
        "Sample Snippets are given below.\n",
        "WITH ranked (\n",
        "    SELECT\n",
        "        e.topic,\n",
        "        e.statement,\n",
        "        e.company,\n",
        "        dense_rank() OVER (\n",
        "            PARTITION BY e.company, e.topic ORDER BY e.probability DESC\n",
        "        ) as rank\n",
        "    FROM esg_reports e\n",
        ")\n",
        "\n",
        "SELECT\n",
        "    t.topic,\n",
        "    t.statement\n",
        "FROM ranked t\n",
        "WHERE t.company = 'goldman sachs'\n",
        "AND t.rank = 1\n",
        "This SQL statement provides us with a NLP generated executive summary for Goldman Sachs. summarising a complex 70+ pages long document into 9 ESG initiatives / actions.\n",
        "\n",
        "\n",
        "{{{{ Sample Snippets are given below.\n",
        "WITH ranked (\n",
        "    SELECT\n",
        "        e.topic,\n",
        "        e.statement,\n",
        "        e.company,\n",
        "        dense_rank() OVER (\n",
        "            PARTITION BY e.company, e.topic ORDER BY e.probability DESC\n",
        "        ) as rank\n",
        "    FROM esg_reports e\n",
        ")\n",
        "\n",
        "SELECT\n",
        "    t.topic,\n",
        "    t.statement\n",
        "FROM ranked t\n",
        "WHERE t.company = 'goldman sachs'\n",
        "AND t.rank = 1\n",
        "}}}}}\n",
        "\n",
        "Using seaborn visualisation, we can easily flag key differences across our companies (organisations' names redacted). When some organisations would put more focus on valuing employees and promoting diversity and inclusion (such as ORG-21), some seem to be more focused towards ethical investments (ORG-14). As the output of LDA is a probability distribution across our 9 topics instead of one specific theme, we easily unveil the most descriptive ESG initiative for any given organisation using a simple SQL statement and a partitioning function that captures the highest probability for each theme.\n",
        "WITH ranked (\n",
        "    SELECT\n",
        "        e.topic,\n",
        "        e.statement,\n",
        "        e.company,\n",
        "        dense_rank() OVER (\n",
        "            PARTITION BY e.company, e.topic ORDER BY e.probability DESC\n",
        "        ) as rank\n",
        "    FROM esg_reports e\n",
        ")\n",
        "\n",
        "SELECT\n",
        "    t.topic,\n",
        "    t.statement\n",
        "FROM ranked t\n",
        "WHERE t.company = 'goldman sachs'\n",
        "AND t.rank = 1\n",
        "This SQL statement provides us with a NLP generated executive summary for Goldman Sachs (see original report), summarising a complex 70+ pages long document into 9 ESG initiatives / actions\n",
        "\n",
        "GDELT\n",
        "Given the volume of data available in GDELT (100 million records for the last 18 months only), we leverage the lakehouse paradigm by moving data from raw, to filtered and enriched, respectively from Bronze, to Silver and Gold layers, and extend our process to operate in near real time (GDELT files are published every 15mn). For that purpose, we use a Structured Streaming approach that we `trigger` in batch mode with each batch operating on data increment only. By unifying Streaming and Batch, Spark is the de-facto standard for data manipulation and ETL processes in modern data lake infrastructures.\n",
        "gdelt_stream_df = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"delta\") \\\n",
        "    .table(\"esg_gdelt_bronze\") \\\n",
        "    .withColumn(\"themes\", filter_themes(F.col(\"themes\"))) \\\n",
        "    .withColumn(\"organisation\", F.explode(F.col(\"organisations\"))) \\\n",
        "    .select(\n",
        "        F.col(\"publishDate\"),\n",
        "        F.col(\"organisation\"),\n",
        "        F.col(\"documentIdentifier\").alias(\"url\"),\n",
        "        F.col(\"themes\"),\n",
        "        F.col(\"tone.tone\")\n",
        "    )\n",
        "\n",
        "gdelt_stream_df \\\n",
        "    .writeStream \\\n",
        "    .trigger(Trigger.Once) \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/gdelt_esg\") \\\n",
        "    .format(\"delta\") \\\n",
        "    .table(\"esg_gdelt_silver\")\n",
        "From the variety of dimensions available in GDELT, we only focus on sentiment analysis (using the tone variable) for financial news related articles only. We assume financial news articles to be well captured by the GDELT taxonomy starting with ECON_*. Furthermore, we assume all environmental articles to be captured as ENV_* and social articles to be captured by UNGP_* taxonomies (UN guiding principles on human rights\n",
        "\n",
        "\n",
        "Given the volume of data available in GDELT (100 million records for the last 18 months only), we leverage the lakehouse paradigm by moving data from raw, to filtered and enriched, respectively from Bronze, to Silver and Gold layers, and extend our process to operate in near real time (GDELT files are published every 15mn). For that purpose, we use a Structured Streaming approach that we `trigger` in batch mode with each batch operating on data increment only. By unifying Streaming and Batch, Spark is the de-facto standard for data manipulation and ETL processes in modern data lake infrastructures.\n",
        "gdelt_stream_df = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"delta\") \\\n",
        "    .table(\"esg_gdelt_bronze\") \\\n",
        "    .withColumn(\"themes\", filter_themes(F.col(\"themes\"))) \\\n",
        "    .withColumn(\"organisation\", F.explode(F.col(\"organisations\"))) \\\n",
        "    .select(\n",
        "        F.col(\"publishDate\"),\n",
        "        F.col(\"organisation\"),\n",
        "        F.col(\"documentIdentifier\").alias(\"url\"),\n",
        "        F.col(\"themes\"),\n",
        "        F.col(\"tone.tone\")\n",
        "    )\n",
        "\n",
        "gdelt_stream_df \\\n",
        "    .writeStream \\\n",
        "    .trigger(Trigger.Once) \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/gdelt_esg\") \\\n",
        "    .format(\"delta\") \\\n",
        "    .table(\"esg_gdelt_silver\")\n",
        "From the variety of dimensions available in GDELT, we only focus on sentiment analysis (using the tone variable) for financial news related articles only. We assume financial news articles to be well captured by the GDELT taxonomy starting with ECON_*. Furthermore, we assume all environmental articles to be captured as ENV_* and social articles to be captured by UNGP_* taxonomies (UN guiding principles on human rights\n",
        "\n",
        "data lake infrastructures.\n",
        "gdelt_stream_df = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"delta\") \\\n",
        "    .table(\"esg_gdelt_bronze\") \\\n",
        "    .withColumn(\"themes\", filter_themes(F.col(\"themes\"))) \\\n",
        "    .withColumn(\"organisation\", F.explode(F.col(\"organisations\"))) \\\n",
        "    .select(\n",
        "        F.col(\"publishDate\"),\n",
        "        F.col(\"organisation\"),\n",
        "        F.col(\"documentIdentifier\").alias(\"url\"),\n",
        "        F.col(\"themes\"),\n",
        "        F.col(\"tone.tone\")\n",
        "    )\n",
        "\n",
        "gdelt_stream_df \\\n",
        "    .writeStream \\\n",
        "    .trigger(Trigger.Once) \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/gdelt_esg\") \\\n",
        "    .format(\"delta\") \\\n",
        "    .table(\"esg_gdelt_silver\")\n",
        "\n",
        "Graphframes\n",
        "\n",
        "\n",
        "Using seaborn visualisation, we can easily flag key differences across our companies (organisations' names redacted). When some organisations would put more focus on valuing employees and promoting diversity and inclusion (such as ORG-21), some seem to be more focused towards ethical investments (ORG-14). As the output of LDA is a probability distribution across our 9 topics instead of one specific theme, we easily unveil the most descriptive ESG initiative for any given organisation using a simple SQL statement and a partitioning function that captures the highest probability for each theme.\n",
        "Sample Snippets are given below.\n",
        "WITH ranked (\n",
        "    SELECT\n",
        "        e.topic,\n",
        "        e.statement,\n",
        "        e.company,\n",
        "        dense_rank() OVER (\n",
        "            PARTITION BY e.company, e.topic ORDER BY e.probability DESC\n",
        "        ) as rank\n",
        "    FROM esg_reports e\n",
        ")\n",
        "\n",
        "SELECT\n",
        "    t.topic,\n",
        "    t.statement\n",
        "FROM ranked t\n",
        "WHERE t.company = 'goldman sachs'\n",
        "AND t.rank = 1\n",
        "This SQL statement provides us with a NLP generated executive summary for Goldman Sachs. summarising a complex 70+ pages long document into 9 ESG initiatives / actions.\n",
        "\n",
        "3.1 Data acquisition\n",
        "Given the volume of data available in GDELT (100 million records for the last 18 months only), we leverage the lakehouse paradigm by moving data from raw, to filtered and enriched, respectively from Bronze, to Silver and Gold layers, and extend our process to operate in near real time (GDELT files are published every 15mn). For that purpose, we use a Structured Streaming approach that we `trigger` in batch mode with each batch operating on data increment only. By unifying Streaming and Batch, Spark is the de-facto standard for data manipulation and ETL processes in modern data lake infrastructures.\n",
        "gdelt_stream_df = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"delta\") \\\n",
        "    .table(\"esg_gdelt_bronze\") \\\n",
        "    .withColumn(\"themes\", filter_themes(F.col(\"themes\"))) \\\n",
        "    .withColumn(\"organisation\", F.explode(F.col(\"organisations\"))) \\\n",
        "    .select(\n",
        "        F.col(\"publishDate\"),\n",
        "        F.col(\"organisation\"),\n",
        "        F.col(\"documentIdentifier\").alias(\"url\"),\n",
        "        F.col(\"themes\"),\n",
        "        F.col(\"tone.tone\")\n",
        "    )\n",
        "\n",
        "gdelt_stream_df \\\n",
        "    .writeStream \\\n",
        "    .trigger(Trigger.Once) \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/gdelt_esg\") \\\n",
        "    .format(\"delta\") \\\n",
        "    .table(\"esg_gdelt_silver\")\n",
        "From the variety of dimensions available in GDELT, we only focus on sentiment analysis (using the tone variable) for financial news related articles only. We assume financial news articles to be well captured by the GDELT taxonomy starting with ECON_*. Furthermore, we assume all environmental articles to be captured as ENV_* and social articles to be captured by UNGP_* taxonomies {Citation:(UN guiding principles on human rights).}\n",
        "\n",
        "\n",
        "\n",
        "3.1 Data acquisition\n",
        "Given the volume of data available in GDELT (100 million records for the last 18 months only), we leverage the lakehouse paradigm by moving data from raw, to filtered and enriched, respectively from Bronze, to Silver and Gold layers, and extend our process to operate in near real time (GDELT files are published every 15mn). For that purpose, we use a Structured Streaming approach that we `trigger` in batch mode with each batch operating on data increment only. By unifying Streaming and Batch, Spark is the de-facto standard for data manipulation and ETL processes in modern data lake infrastructures.\n",
        "gdelt_stream_df = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"delta\") \\\n",
        "    .table(\"esg_gdelt_bronze\") \\\n",
        "    .withColumn(\"themes\", filter_themes(F.col(\"themes\"))) \\\n",
        "    .withColumn(\"organisation\", F.explode(F.col(\"organisations\"))) \\\n",
        "    .select(\n",
        "        F.col(\"publishDate\"),\n",
        "        F.col(\"organisation\"),\n",
        "        F.col(\"documentIdentifier\").alias(\"url\"),\n",
        "        F.col(\"themes\"),\n",
        "        F.col(\"tone.tone\")\n",
        "    )\n",
        "\n",
        "gdelt_stream_df \\\n",
        "    .writeStream \\\n",
        "    .trigger(Trigger.Once) \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/gdelt_esg\") \\\n",
        "    .format(\"delta\") \\\n",
        "    .table(\"esg_gdelt_silver\")\n",
        "From the variety of dimensions available in GDELT, we only focus on sentiment analysis (using the tone variable) for financial news related articles only. We assume financial news articles to be well captured by the GDELT taxonomy starting with ECON_*. Furthermore, we assume all environmental articles to be captured as ENV_* and social articles to be captured by UNGP_* taxonomies {Citation:(UN guiding principles on human rights).}\n",
        "3.2.Sentiment analysis as proxy for ESG\n",
        "Without any industry standard nor existing models to define environmental, social and governance metrics, and without any ground truth availability at the time of this study, its  assumed that the overall tone captured from financial news articles is a good proxy for companies' ESG scores.\n",
        "\n",
        "\n",
        "Using Graphframes, [Citation:]we can easily create a network of companies sharing a common media coverage. Our assumption is that the more companies are mentioned together in news articles, the stronger their link will be (edge weight). Although this assumption may also infer wrong connections because of random co-occurrence in news articles, this undirected weighted graph will help us find companies' importance relative to our core FSIs we would like to assess.\n",
        "Sample code frame:\n",
        "val buildTuples = udf((organisations: Seq[String]) => {\n",
        "    // as undirected, we create both IN and OUT connections\n",
        "    organisations.flatMap(x1 => {\n",
        "        organisations.map(x2 => {\n",
        "        (x1, x2)\n",
        "        })\n",
        "    }).toSeq.filter({ case (x1, x2) =>\n",
        "        x1 != x2 // remove self edges\n",
        "    })\n",
        "})\n",
        "\n",
        "val edges = spark.read.table(\"esg_gdelt\")\n",
        "    .groupBy(\"url\")\n",
        "    .agg(collect_list(col(\"organisation\")).as(\"organisations\"))\n",
        "    .withColumn(\"tuples\", buildTuples(col(\"organisations\")))\n",
        "    .withColumn(\"tuple\", explode(col(\"tuples\")))\n",
        "    .withColumn(\"src\", col(\"tuple._1\"))\n",
        "    .withColumn(\"dst\", col(\"tuple._2\"))\n",
        "    .groupBy(\"src\", \"dst\")\n",
        "    .count()\n",
        "\n",
        "import org.graphframes.GraphFrame\n",
        "val esgGraph = GraphFrame(nodes, edges)\n",
        "\n",
        "The depth of a graph is the maximum of every shortest path possible, or the number of connections it takes for any random node to reach any others (the smaller the depth is, denser is our network).\n",
        "Sample code frame:\n",
        "val shortestPaths = esgGraph.shortestPaths.landmarks(landmarks).run()\n",
        "val filterDepth = udf((distances: Map[String, Int]) => {\n",
        "    distances.values.exists(_ )\n",
        "\n",
        "We filter the  graph under consideration, to have a maximum depth of 4. This process reduces our graph further down to 2,300 businesses and 54,000 connections, allowing to run Page Rank algorithm more extensively with more iterations in order to better capture industry influence.\n",
        "\n",
        "val prNodes = esgDenseGraph .parallelPersonalizedPageRank .maxIter(100) .sourceIds(landmarks) .run()\n",
        "\n",
        "Fig:3: ESG Score variations with industry\n",
        "Using Graphframes, [Citation:]we can easily create a network of companies sharing a common media coverage. Our assumption is that the more companies are mentioned together in news articles, the stronger their link will be (edge weight). Although this assumption may also infer wrong connections because of random co-occurrence in news articles, this undirected weighted graph will help us find companies' importance relative to our core FSIs we would like to assess.\n",
        "Sample code frame:\n",
        "val buildTuples = udf((organisations: Seq[String]) => {\n",
        "    // as undirected, we create both IN and OUT connections\n",
        "    organisations.flatMap(x1 => {\n",
        "        organisations.map(x2 => {\n",
        "        (x1, x2)\n",
        "        })\n",
        "    }).toSeq.filter({ case (x1, x2) =>\n",
        "        x1 != x2 // remove self edges\n",
        "    })\n",
        "})\n",
        "\n",
        "val edges = spark.read.table(\"esg_gdelt\")\n",
        "    .groupBy(\"url\")\n",
        "    .agg(collect_list(col(\"organisation\")).as(\"organisations\"))\n",
        "    .withColumn(\"tuples\", buildTuples(col(\"organisations\")))\n",
        "    .withColumn(\"tuple\", explode(col(\"tuples\")))\n",
        "    .withColumn(\"src\", col(\"tuple._1\"))\n",
        "    .withColumn(\"dst\", col(\"tuple._2\"))\n",
        "    .groupBy(\"src\", \"dst\")\n",
        "    .count()\n",
        "\n",
        "import org.graphframes.GraphFrame\n",
        "val esgGraph = GraphFrame(nodes, edges)\n",
        "By studying this graph further, we observe a power of law distribution of its edge weights: 90% of the connected businesses share a very few connections. We drastically reduce the graph size from 51,679,930 down to 61,143 connections by filtering edges for a weight of 200 or above (empirically led threshold). Prior to running Page Rank, we also optimise our graph by further reducing the number of connections through a Shortest Path[Citation] algorithm and compute the maximum number of hops a node needs to follow to reach any of our core FSIs vertices (captured in `landmarks` array). The depth of a graph is the maximum of every shortest path possible, or the number of connections it takes for any random node to reach any others (the smaller the depth is, denser is our network).\n",
        "Sample code frame:\n",
        "val shortestPaths = esgGraph.shortestPaths.landmarks(landmarks).run()\n",
        "val filterDepth = udf((distances: Map[String, Int]) => {\n",
        "    distances.values.exists(_ )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9BqnR5Cl2gSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1sW1lf7q2gyk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}